# **USDT-RL: Online Reinforcement Learning Engine for K-Premia**

> í•´ë‹¹ í”„ë¡œì íŠ¸ëŠ” **Python 3.11** ê¸°ë°˜ì˜ **Online Reinforcement Learning (RL)** ì—ì´ì „íŠ¸ ì—”ì§„ìœ¼ë¡œ,  
> ê¹€ì¹˜í”„ë¦¬ë¯¸ì—„(K-Premia) ë° í™˜ìœ¨ ë³€ë™ì„ ì´ìš©í•œ **USDTâ€“KRW ë‹¨íƒ€ ì „ëµ**ì„  
> ì‹¤ì‹œê°„ìœ¼ë¡œ í•™ìŠµí•˜ê³  ì ì‘í•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.  
>  
> ê±°ë˜ í™˜ê²½(Environment, K-Premia ëª¨ë“ˆ)ìœ¼ë¡œë¶€í„° ìƒíƒœ(state)ë¥¼ ì£¼ê¸°ì ìœ¼ë¡œ ì…ë ¥ë°›ì•„,  
> GRU + Attention ê¸°ë°˜ì˜ ì •ì±… ì‹ ê²½ë§ì„ í†µí•´ í–‰ë™(action)ì„ ì‚°ì¶œí•˜ê³ ,  
> ê·¸ ê²°ê³¼ë¥¼ ì‹¤ì‹œê°„ ë³´ìƒ(reward)ìœ¼ë¡œ ë°˜ì˜í•˜ì—¬ ì§€ì†ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.  
>  
> ì •ì±…ì€ ê³ ì •ëœ ê·œì¹™ ê¸°ë°˜ì´ ì•„ë‹ˆë¼,  
> **ì‹œì¥ ë³€ë™ì„±ê³¼ ë¦¬ìŠ¤í¬ ìš”ì¸ì— ë”°ë¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ì¬í•™ìŠµ(adaptive fine-tuning)** ë˜ëŠ” êµ¬ì¡°ì…ë‹ˆë‹¤.

---

### âœ³ï¸ ì£¼ìš” íŠ¹ì§•
- **Online RL Architecture:** ì‹œë®¬ë ˆì´ì…˜ê³¼ ì‹¤ê±°ë˜ ì¤‘ ëª¨ë‘ í•™ìŠµ ê°€ëŠ¥  
- **Dual-Timescale Input:** 15ë¶„ë´‰ / ì¼ë´‰ / ê¹€í”„ ì‹œê³„ì—´ ë³‘ë ¬ ì¸ì½”ë”©  
- **Attention-Based Fusion:** ì‹œê°„ì  ì¤‘ìš”ë„ì— ë”°ë¥¸ ë™ì  ê°€ì¤‘  
- **Policy / Value Head ë¶„ë¦¬:** Softmax(2) + Value(1) êµ¬ì¡°  
- **Lightweight Integration:** ì„œë²„ ì—†ì´ ë¡œì»¬ í™˜ê²½(env)ê³¼ ì§ì ‘ ì—°ë™  

---

### ğŸ§­ í”„ë¡œì íŠ¸ ëª©ì 
- ê¹€ì¹˜í”„ë¦¬ë¯¸ì—„ ë° í™˜ìœ¨ì˜ ë™ì  ë³€í™”ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ë°˜ì˜í•´  
  **ë‹¨ê¸° ì‹œì¥ ì™œê³¡(temporal arbitrage)** ì„ í¬ì°©í•˜ê³  ë°˜ì‘í•˜ëŠ” ê°•í™”í•™ìŠµ ê¸°ë°˜ ì˜ì‚¬ê²°ì • ì—”ì§„ ê°œë°œ  
- ë‹¨ìˆœí•œ ì˜ˆì¸¡í˜• ëª¨ë¸ì´ ì•„ë‹ˆë¼,  
  **í–‰ë™ ê¸°ë°˜ í”¼ë“œë°± ë£¨í”„(actionâ€“rewardâ€“update)** ë¥¼ í†µí•´  
  **ìŠ¤ìŠ¤ë¡œ ì ì‘í•˜ëŠ” ê±°ë˜ ì •ì±…(self-adaptive trading policy)** êµ¬í˜„

---

## ğŸ“‚ í”„ë¡œì íŠ¸ êµ¬ì¡°

```text
agent/
â”œâ”€â”€ config.yaml                     # ê³µí†µ ì„¤ì • (model/ppo/training/policy/data)
â”œâ”€â”€ run_sim.py                      # ì‹œë®¬ í•™ìŠµ ì—”íŠ¸ë¦¬í¬ì¸íŠ¸
â”œâ”€â”€ run_live.py                     # ì‹¤ì „(ì˜¨ë¼ì¸ RL) ì—”íŠ¸ë¦¬í¬ì¸íŠ¸
â”‚
â”œâ”€â”€ core/                           # ğŸ’  ì—ì´ì „íŠ¸ ì½”ì–´ (ì‹œë®¬/ì‹¤ì „ ê³µí†µ)
â”‚   â”œâ”€â”€ agent_core.py               # RLAgent: act()/compute_reward()/learn() + ë²„í¼ ê´€ë¦¬
â”‚   â”œâ”€â”€ model.py                    # GRU + Attention + Policy(2) / Value(1)
â”‚   â”œâ”€â”€ ppo_core.py                 # PPO í´ë¦½ ì†ì‹¤/ì—”íŠ¸ë¡œí”¼/ê°’í•¨ìˆ˜ + optimizer
â”‚   â”œâ”€â”€ memory.py                   # ê°„ë‹¨í•œ ReplayBuffer(rolling)
â”‚   â””â”€â”€ utils.py                    # log(), load_config(), save_path() ë“± ìœ í‹¸
â”‚
â”œâ”€â”€ sim/                            # ğŸ§  ì‹œë®¬ë ˆì´ì…˜(í›ˆë ¨) ì „ìš©
â”‚   â”œâ”€â”€ simulator.py                # ê³¼ê±°ë°ì´í„° ê¸°ë°˜ ê°€ìƒì²´ê²° + ìƒíƒœ/metrics ë°˜í™˜
â”‚   â””â”€â”€ trainer.py                  # ì‹œë®¬ í•™ìŠµ ë£¨í”„(Agentâ†”Simulator ì—°ê²°)
â”‚
â””â”€â”€ live/                           # âš¡ ì‹¤ì „ ë£¨í”„(í”¼ë“œ/ì‹¤í–‰ì€ ì™¸ë¶€ ëª¨ë“ˆ ì£¼ì…)
    â””â”€â”€ live_loop.py                # get_live_state/execute_action ì£¼ì…í˜• ì˜¨ë¼ì¸ RL ë£¨í”„
```


---

## âš™ï¸ ì£¼ìš” ê¸°ëŠ¥

### `/act`
- ì…ë ¥: ìµœê·¼ ì‹œì„¸, ê³„ì¢Œ ìƒíƒœ, ê¹€í”„ ë“±  
- ì¶œë ¥: `action (0=Hold, 1=Action)`  
- ì‚¬ìš©ì²˜: ì‹¤ì „/ì‹œë®¬ ëª¨ë‘ ë™ì¼

### `/learn`
- ì…ë ¥: transition mini-batch  
- ì²˜ë¦¬: PPO ì—…ë°ì´íŠ¸  
- ì¶œë ¥: ì—…ë°ì´íŠ¸ ë¡œê·¸ (loss, reward í‰ê·  ë“±)

### `/save`, `/load`
- ì •ì±… ë²„ì „ ê´€ë¦¬ (`/checkpoints/v1_0_0.pt` ë“±)

### `/set_hparams`
- íƒí—˜ë¥ (temperature), lr, clip ë“± ëŸ°íƒ€ì„ ì¡°ì •

---

## ğŸ§  Model Architecture


```text
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Input:
 â”œâ”€ 15m close series  â†’ GRU + Attention Pooling
 â”œâ”€ 1d close series   â†’ GRU + Attention Pooling
 â”œâ”€ 1d Kimchi Premium    â†’ GRU + Attention Pooling
 â”œâ”€ Account State     â†’ Dense Encoding (KRW, USDT)
 â””â”€ Current Price     â†’ Scalar Input

Fusion:
 â””â”€ Concat + Dense â†’
      â”œâ”€ Policy Head â†’ Softmax(2)  (Hold / Trade)
      â””â”€ Value Head  â†’ Linear(1)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```


## Action semantics
```text
0: HOLD

1: SIGNAL (â†’ ê³„ì¢Œ ë¹„ì¤‘ì— ë”°ë¼ BUY / SELLë¡œ ìë™ í•´ì„)
```



---

## ğŸ§© í•™ìŠµ ì„¤ì •

| Category         | Key                            | Default          | Description                        |
| ---------------- | ------------------------------ | ---------------- | ---------------------------------- |
| **Model**        | `model.hidden_dim`             | `128`            | GRU hidden state í¬ê¸°                |
|                  | `model.num_layers`             | `1`              | GRU layer ìˆ˜                        |
|                  | `model.dropout`                | `0.1`            | GRU dropout ë¹„ìœ¨                     |
|                  | `model.attn_dim`               | `64`             | Attention pooling ì°¨ì›               |
| **PPO**          | `ppo.clip_ratio`               | `0.2`            | PPO ì •ì±… í´ë¦¬í•‘ í•œê³„                      |
|                  | `ppo.lr`                       | `3e-4`           | Adam í•™ìŠµë¥                            |
|                  | `ppo.value_coef`               | `0.5`            | Value ì†ì‹¤ ê°€ì¤‘ì¹˜                       |
|                  | `ppo.entropy_coef`             | `0.01`           | íƒí—˜(ì—”íŠ¸ë¡œí”¼) ë³´ë„ˆìŠ¤ ê°€ì¤‘ì¹˜                   |
| **Training**     | `training.batch_size`          | `64`             | í•™ìŠµ ë°°ì¹˜ í¬ê¸°                           |
|                  | `training.epochs`              | `10`             | í•œ ì—í¬í¬ë‹¹ ì—…ë°ì´íŠ¸ ë°˜ë³µ ìˆ˜                   |
|                  | `training.gamma`               | `0.99`           | í• ì¸ìœ¨ (reward decay factor)          |
|                  | `training.update_freq`         | `5`              | ëª‡ ìŠ¤í…ë§ˆë‹¤ í•™ìŠµí• ì§€ (ë²„í¼ ê¸¸ì´ ê¸°ì¤€)             |
|                  | `training.device`              | `"cuda"`         | ê¸°ë³¸ ì—°ì‚° ë””ë°”ì´ìŠ¤                         |
| **Data Window**  | `data.window_15m`              | `20`             | 15ë¶„ë´‰ ì…ë ¥ ì‹œí€€ìŠ¤ ê¸¸ì´                     |
|                  | `data.window_1d`               | `8`              | ì¼ë´‰ ì…ë ¥ ì‹œí€€ìŠ¤ ê¸¸ì´                       |
| **Policy Logic** | `policy.signal_sell_threshold` | `0.10`           | USDT ì”ê³  ë¹„ì¤‘ì´ 10% ì´ìƒì¼ ê²½ìš° SIGNAL=SELL |
| **Misc**         | `versioning.save_dir`          | `"checkpoints/"` | ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ê²½ë¡œ                        |
|                  | `versioning.auto_timestamp`    | `true`           | íƒ€ì„ìŠ¤íƒ¬í”„ë³„ í´ë” ìë™ ìƒì„±                    |


---

## ğŸš€ ì‹¤í–‰ ë°©ë²•

1. **Simulated Training**
   ```python
   python run_sim.py
   ```
2. **Live (Online RL)**
   ```python
   python run_live.py
   ```

---

## ğŸ§‘â€ğŸ’» Author

- **ì´ìŠ¹ì—½** â€” RL ë¸Œë ˆì¸ ì„¤ê³„ ë° ì—°êµ¬

---

> _â€œA demon of capitalism, swimming the net and devouring volatility.â€_
