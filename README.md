# **USDT-RL: Online Reinforcement Learning Engine for K-Premia**

> í•´ë‹¹ í”„ë¡œì íŠ¸ëŠ” **Python 3.11** ê¸°ë°˜ì˜ **Online Reinforcement Learning (RL)** ì—ì´ì „íŠ¸ ì—”ì§„ìœ¼ë¡œ,  
> ê¹€ì¹˜í”„ë¦¬ë¯¸ì—„(K-Premia) ë° í™˜ìœ¨ ë³€ë™ì„ ì´ìš©í•œ **USDTâ€“KRW ë‹¨íƒ€ ì „ëµ**ì„  
> ì‹¤ì‹œê°„ìœ¼ë¡œ í•™ìŠµí•˜ê³  ì ì‘í•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.  
>  
> ê±°ë˜ í™˜ê²½(Environment, K-Premia ëª¨ë“ˆ)ìœ¼ë¡œë¶€í„° ìƒíƒœ(state)ë¥¼ ì£¼ê¸°ì ìœ¼ë¡œ ì…ë ¥ë°›ì•„,  
> GRU + Attention ê¸°ë°˜ì˜ ì •ì±… ì‹ ê²½ë§ì„ í†µí•´ í–‰ë™(action)ì„ ì‚°ì¶œí•˜ê³ ,  
> ê·¸ ê²°ê³¼ë¥¼ ì‹¤ì‹œê°„ ë³´ìƒ(reward)ìœ¼ë¡œ ë°˜ì˜í•˜ì—¬ ì§€ì†ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.  
>  
> ì •ì±…ì€ ê³ ì •ëœ ê·œì¹™ ê¸°ë°˜ì´ ì•„ë‹ˆë¼,  
> **ì‹œì¥ ë³€ë™ì„±ê³¼ ë¦¬ìŠ¤í¬ ìš”ì¸ì— ë”°ë¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ì¬í•™ìŠµ(adaptive fine-tuning)** ë˜ëŠ” êµ¬ì¡°ì…ë‹ˆë‹¤.

---

### âœ³ï¸ ì£¼ìš” íŠ¹ì§•

- **ğŸ§  Online PPO Architecture**  
  ì‹œë®¬ë ˆì´ì…˜ í™˜ê²½ê³¼ ì‹¤ê±°ë˜ í™˜ê²½ì„ ëª¨ë‘ ì§€ì›í•˜ëŠ” **Online Reinforcement Learning êµ¬ì¡°**.  
  í•™ìŠµëœ ì •ì±…ì€ ì‹¤ì‹œê°„ ì‹œì¥ ë³€í™”ì— ì ì‘í•˜ë©°, ë²„í¼ì— ìˆ˜ì§‘ëœ ê²½í—˜(transition)ì„ ê¸°ë°˜ìœ¼ë¡œ  
  **Proximal Policy Optimization (PPO)** ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ì§€ì†ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë¨.

- **â±ï¸ Dual-Timescale Input Encoding**  
  15ë¶„ë´‰(`price_15m`), ì¼ë´‰(`price_1d`), ê¹€ì¹˜ í”„ë¦¬ë¯¸ì—„(`kimchi_premium`) ë“±  
  ì„œë¡œ ë‹¤ë¥¸ ì‹œê°„ ìŠ¤ì¼€ì¼ì˜ ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ë³‘ë ¬ ì¸ì½”ë”©í•˜ì—¬ **ë‹¨ê¸°/ì¤‘ê¸° ì‹œì¥ êµ¬ì¡°ë¥¼ ë™ì‹œì— ë°˜ì˜**.  
  ê° ì‹œê³„ì—´ì€ ë…ë¦½ì ì¸ GRU ì¸ì½”ë”ë¥¼ ê±°ì³ latent featureë¡œ ë³€í™˜ë˜ë©°,  
  ì´ë“¤ì´ Attention ëª¨ë“ˆì—ì„œ í†µí•©ë˜ì–´ ì‹œì ë³„ ìƒëŒ€ì  ì¤‘ìš”ë„ë¥¼ í•™ìŠµí•¨.

- **ğŸ¯ GRU + Attention Fusion**  
  GRUì˜ ìˆœì°¨ì  ê¸°ì–µ(Long-term dependency)ê³¼  
  Attentionì˜ ë¹„ì„ í˜• ì¤‘ìš”ë„ ì¡°í•©ì„ ê²°í•©í•˜ì—¬  
  **ì‹œì¥ ìƒíƒœì˜ ì‹œì ë³„ ì¤‘ìš”ë„(weighted temporal context)** ë¥¼ í•™ìŠµ.  
  ë‹¨ìˆœí•œ ì‹œê³„ì—´ ì…ë ¥ì—ì„œë„ êµ¬ì¡°ì  feature extractionì´ ê°€ëŠ¥í•¨.

- **ğŸ“Š Feature-Augmented Long-Term Memory**  
  ëª¨ë¸ì´ ëª¨ë“  ê³¼ê±° ë°ì´í„°ë¥¼ ì§ì ‘ â€œê¸°ì–µâ€í•˜ì§€ ì•Šê³ ,  
  ì´ë™í‰ê· , ë³€ë™ì„±, ëª¨ë©˜í…€ ë“± **í†µê³„ì  feature ìš”ì•½ê°’ì„ state dictì— í•¨ê»˜ ì œê³µ**.  
  ì´ë¥¼ í†µí•´ **ì¥ê¸° ì‹œì¥ êµ­ë©´(regime) ì •ë³´**ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ë‚´ì¬í™”í•˜ê³   
  í•™ìŠµ ì•ˆì •ì„±ê³¼ ì¼ë°˜í™” ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚´.

- **ğŸ”€ Policy / Value Head Separation**  
  ê³µí†µ feature backbone ì´í›„,  
  ì •ì±… í™•ë¥ (Softmax(2))ê³¼ ìƒíƒœ ê°€ì¹˜(Value(1))ë¥¼ ë¶„ë¦¬ ì¶œë ¥í•˜ëŠ” **Actorâ€“Critic êµ¬ì¡°**.  
  PPOì˜ clipping objective + value loss + entropy regularizationì„ ì ìš©í•´  
  **íƒí—˜(Exploration)** ê³¼ **ìˆ˜ë ´(Stability)** ì˜ ê· í˜•ì„ ìœ ì§€í•¨.

- **âš¡ Lightweight Local Integration**  
  ì™¸ë¶€ ì„œë²„ë‚˜ API ì—†ì´ ë¡œì»¬ í™˜ê²½(`simulator`)ê³¼ ì§ì ‘ ì—°ë™ ê°€ëŠ¥.  
  ë™ì¼í•œ ì½”ë“œ ê²½ë¡œì—ì„œ ì‹œë®¬ë ˆì´ì…˜(ë°±í…ŒìŠ¤íŠ¸)ê³¼ ì‹¤ì‹œê°„ ê±°ë˜ í™˜ê²½ì„ ëª¨ë‘ ì§€ì›í•˜ë©°,  
  **Online RL ê¸°ë°˜ ë‹¨íƒ€ íŠ¸ë ˆì´ë”© ì—ì´ì „íŠ¸**ë¡œ í™•ì¥ ê°€ëŠ¥.

---

### ğŸ§­ í”„ë¡œì íŠ¸ ëª©ì 
- ê¹€ì¹˜í”„ë¦¬ë¯¸ì—„ ë° í™˜ìœ¨ì˜ ë™ì  ë³€í™”ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ë°˜ì˜í•´  
  **ë‹¨ê¸° ì‹œì¥ ì™œê³¡(temporal arbitrage)** ì„ í¬ì°©í•˜ê³  ë°˜ì‘í•˜ëŠ” ê°•í™”í•™ìŠµ ê¸°ë°˜ ì˜ì‚¬ê²°ì • ì—”ì§„ ê°œë°œ  
- ë‹¨ìˆœí•œ ì˜ˆì¸¡í˜• ëª¨ë¸ì´ ì•„ë‹ˆë¼,  
  **í–‰ë™ ê¸°ë°˜ í”¼ë“œë°± ë£¨í”„(actionâ€“rewardâ€“update)** ë¥¼ í†µí•´  
  **ìŠ¤ìŠ¤ë¡œ ì ì‘í•˜ëŠ” ê±°ë˜ ì •ì±…(self-adaptive trading policy)** êµ¬í˜„

---

## ğŸ“‚ í”„ë¡œì íŠ¸ êµ¬ì¡°

```text
agent/
â”œâ”€â”€ config.yaml                     # ê³µí†µ ì„¤ì • (model/ppo/training/policy/data)
â”œâ”€â”€ run_sim.py                      # ì‹œë®¬ í•™ìŠµ ì—”íŠ¸ë¦¬í¬ì¸íŠ¸
â”œâ”€â”€ run_live.py                     # ì‹¤ì „(ì˜¨ë¼ì¸ RL) ì—”íŠ¸ë¦¬í¬ì¸íŠ¸
â”‚
â”œâ”€â”€ core/                           # ğŸ’  ì—ì´ì „íŠ¸ ì½”ì–´ (ì‹œë®¬/ì‹¤ì „ ê³µí†µ)
â”‚   â”œâ”€â”€ agent_core.py               # RLAgent: act()/compute_reward()/learn() + ë²„í¼ ê´€ë¦¬
â”‚   â”œâ”€â”€ model.py                    # GRU + Attention + Policy(2) / Value(1)
â”‚   â”œâ”€â”€ ppo_core.py                 # PPO í´ë¦½ ì†ì‹¤/ì—”íŠ¸ë¡œí”¼/ê°’í•¨ìˆ˜ + optimizer
â”‚   â”œâ”€â”€ memory.py                   # ê°„ë‹¨í•œ ReplayBuffer(rolling)
â”‚   â””â”€â”€ utils.py                    # log(), load_config(), save_path() ë“± ìœ í‹¸
â”‚
â”œâ”€â”€ sim/                            # ğŸ§  ì‹œë®¬ë ˆì´ì…˜(í›ˆë ¨) ì „ìš©
â”‚   â”œâ”€â”€ simulator.py                # ê³¼ê±°ë°ì´í„° ê¸°ë°˜ ê°€ìƒì²´ê²° + ìƒíƒœ/metrics ë°˜í™˜
â”‚   â””â”€â”€ trainer.py                  # ì‹œë®¬ í•™ìŠµ ë£¨í”„(Agentâ†”Simulator ì—°ê²°)
â”‚
â”œâ”€â”€ live/                           # âš¡ ì‹¤ì „ ë£¨í”„(í”¼ë“œ/ì‹¤í–‰ì€ ì™¸ë¶€ ëª¨ë“ˆ ì£¼ì…)
â”‚   â””â”€â”€ live_loop.py                # get_live_state/execute_action ì£¼ì…í˜• ì˜¨ë¼ì¸ RL ë£¨í”„
â””â”€â”€ io/
    â”œâ”€â”€ market_data.py              # ì‹œì¥ ìº”ë“¤, í™˜ìœ¨ ë°ì´í„° ìˆ˜ì§‘
    â””â”€â”€ trade_engine.py             # ê³„ì¢Œì •ë³´, ê±°ë˜ì²´ê²°
```


---

## âš™ï¸ ì£¼ìš” ê¸°ëŠ¥

### `/act`
- ì…ë ¥: ìµœê·¼ ì‹œì„¸, ê³„ì¢Œ ìƒíƒœ, ê¹€í”„ ë“±  
- ì¶œë ¥: `action (0=Hold, 1=Action)`  
- ì‚¬ìš©ì²˜: ì‹¤ì „/ì‹œë®¬ ëª¨ë‘ ë™ì¼

### `/learn`
- ì…ë ¥: transition mini-batch  
- ì²˜ë¦¬: PPO ì—…ë°ì´íŠ¸  
- ì¶œë ¥: ì—…ë°ì´íŠ¸ ë¡œê·¸ (loss, reward í‰ê·  ë“±)

### `/save`, `/load`
- ì •ì±… ë²„ì „ ê´€ë¦¬ (`/checkpoints/v1_0_0.pt` ë“±)

### `/set_hparams`
- íƒí—˜ë¥ (temperature), lr, clip ë“± ëŸ°íƒ€ì„ ì¡°ì •

---

## ğŸ§  Model Architecture


```text
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Input:
 â”œâ”€ 15m close series  â†’ GRU + Attention Pooling
 â”œâ”€ 1d close series   â†’ GRU + Attention Pooling
 â”œâ”€ 1d Kimchi Premium    â†’ GRU + Attention Pooling
 â”œâ”€ Account State     â†’ Dense Encoding (KRW, USDT)
 â””â”€ Current Price     â†’ Scalar Input

Fusion:
 â””â”€ Concat + Dense â†’
      â”œâ”€ Policy Head â†’ Softmax(2)  (Hold / Trade)
      â””â”€ Value Head  â†’ Linear(1)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```


## Action semantics
```text
0: HOLD

1: SIGNAL (â†’ ê³„ì¢Œ ë¹„ì¤‘ì— ë”°ë¼ BUY / SELLë¡œ ìë™ í•´ì„)
```



---

## ğŸ§© í•™ìŠµ ì„¤ì •

| **Category**                 | **Key**                         | **Default / Example** | **Description**                |
| ---------------------------- | ------------------------------- | --------------------- | ------------------------------ |
| **Model**                    | `model.hidden_dim`              | `64`                  | GRU hidden state í¬ê¸°            |
|                              | `model.gru_layers`              | `1`                   | GRU layer ìˆ˜                    |
|                              | `model.dropout`                 | `0.1`                 | Dropout ë¹„ìœ¨                     |
|                              | `model.attention`               | `true`                | Attention pooling ì‚¬ìš© ì—¬ë¶€        |
|                              | `model.action_dim`              | `2`                   | í–‰ë™ ì°¨ì› (`BUY`, `SELL`)          |
| **PPO (ê³µí†µ)**                 | `ppo.gamma`                     | `0.99`                | Discount factor (ë³´ìƒ ê°ì‡ ìœ¨)       |
| **Training (ê³µí†µ)**            | `training.device`               | `"cuda"`              | ì—°ì‚° ë””ë°”ì´ìŠ¤ (`cpu`/`cuda`)         |
|                              | `training.buffer_maxlen`        | `10000`               | ê²½í—˜ ë²„í¼ ìµœëŒ€ í¬ê¸°                    |
|                              | `training.save_dir`             | `"checkpoints/"`      | ëª¨ë¸ ì €ì¥ ê²½ë¡œ                       |
|                              | `training.save_interval_steps`  | `500`                 | ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì£¼ê¸°(step ë‹¨ìœ„)           |
| **Training â€“ Simulation**    | `training.sim.lr`               | `3e-4`                | ì‹œë®¬ë ˆì´ì…˜ í•™ìŠµë¥                       |
|                              | `training.sim.clip_epsilon`     | `0.2`                 | PPO í´ë¦¬í•‘ í•œê³„                     |
|                              | `training.sim.entropy_coef`     | `0.01`                | ì—”íŠ¸ë¡œí”¼(íƒí—˜) ê°€ì¤‘ì¹˜                   |
|                              | `training.sim.value_coef`       | `0.5`                 | Value ì†ì‹¤ ê°€ì¤‘ì¹˜                   |
|                              | `training.sim.update_epochs`    | `3`                   | í•™ìŠµ ë°˜ë³µ(epoch) ìˆ˜                 |
|                              | `training.sim.update_interval`  | `64`                  | ì—…ë°ì´íŠ¸ ì£¼ê¸° (ë²„í¼ ê¸¸ì´ ê¸°ì¤€)             |
|                              | `training.sim.update_freq`      | `5`                   | í•™ìŠµ í˜¸ì¶œ ê°„ê²© (step ê¸°ì¤€)             |
| **Training â€“ Live (Online)** | `training.live.lr`              | `1e-4`                | ì‹¤ì „ìš© í•™ìŠµë¥                         |
|                              | `training.live.clip_epsilon`    | `0.1`                 | PPO í´ë¦¬í•‘ í•œê³„                     |
|                              | `training.live.entropy_coef`    | `0.02`                | ì—”íŠ¸ë¡œí”¼(íƒí—˜) ê°€ì¤‘ì¹˜                   |
|                              | `training.live.value_coef`      | `0.4`                 | Value ì†ì‹¤ ê°€ì¤‘ì¹˜                   |
|                              | `training.live.update_epochs`   | `1`                   | ì˜¨ë¼ì¸ ì—…ë°ì´íŠ¸ ë°˜ë³µ ìˆ˜                  |
|                              | `training.live.update_interval` | `16`                  | ì—…ë°ì´íŠ¸ ì£¼ê¸° (ë²„í¼ ê¸¸ì´ ê¸°ì¤€)             |
|                              | `training.live.update_freq`     | `3`                   | í•™ìŠµ í˜¸ì¶œ ê°„ê²© (step ê¸°ì¤€)             |
| **Live Execution**           | `live.refresh_interval`         | `10`                  | ì‹¤ì‹œê°„ ë°ì´í„° ì—…ë°ì´íŠ¸ ê°„ê²©(ì´ˆ)             |
| **Policy Logic**             | `policy.signal_sell_threshold`  | `0.10`                | USDT ì”ê³  ë¹„ì¤‘ì´ 10% ì´ìƒì´ë©´ `SELL` ì‹ í˜¸ |
| **Data Window**              | `data.window_15m`               | `15`                  | 15ë¶„ë´‰ ì…ë ¥ ì‹œí€€ìŠ¤ ê¸¸ì´                 |
|                              | `data.window_1d`                | `10`                  | ì¼ë´‰ ì…ë ¥ ì‹œí€€ìŠ¤ ê¸¸ì´                   |
| **Simulation Env**           | `sim.max_steps`                 | `1000`                | ì‹œë®¬ ìµœëŒ€ step ìˆ˜                   |
|                              | `sim.fee`                       | `0.001`               | ê±°ë˜ ìˆ˜ìˆ˜ë£Œ ë¹„ìœ¨                      |
|                              | `sim.init_krw`                  | `1000000`             | ì´ˆê¸° ìë³¸ (KRW)                    |
| **Versioning**               | `versioning.save_dir`           | `"checkpoints/"`      | ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ê²½ë¡œ                    |
|                              | `versioning.auto_timestamp`     | `true`                | íƒ€ì„ìŠ¤íƒ¬í”„ë³„ í´ë” ìë™ ìƒì„± ì—¬ë¶€             |



---

## ğŸš€ ì‹¤í–‰ ë°©ë²•

1. **Simulated Training**
   ```python
   python run_sim.py
   ```
2. **Live (Online RL)**
   ```python
   python run_live.py
   ```

---

## ğŸ§‘â€ğŸ’» Author

- **ì´ìŠ¹ì—½** â€” RL ë¸Œë ˆì¸ ì„¤ê³„ ë° ì—°êµ¬

---

> _â€œA demon of capitalism, swimming the net and devouring volatility.â€_
