# **USDT-RL: Online Reinforcement Learning Engine for K-Premia**

> í•´ë‹¹ í”„ë¡œì íŠ¸ëŠ” **Python 3.11** ê¸°ë°˜ì˜ **Online Reinforcement Learning (RL)** ì—ì´ì „íŠ¸ ì—”ì§„ìœ¼ë¡œ,  
> ê¹€ì¹˜í”„ë¦¬ë¯¸ì—„(K-Premia) ë° í™˜ìœ¨ ë³€ë™ì„ ì´ìš©í•œ **USDTâ€“KRW ë‹¨íƒ€ ì „ëµ**ì„  
> ì‹¤ì‹œê°„ìœ¼ë¡œ í•™ìŠµí•˜ê³  ì ì‘í•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.  
>  
> ê±°ë˜ í™˜ê²½(Environment, K-Premia ëª¨ë“ˆ)ìœ¼ë¡œë¶€í„° ìƒíƒœ(state)ë¥¼ ì£¼ê¸°ì ìœ¼ë¡œ ì…ë ¥ë°›ì•„,  
> GRU + Attention ê¸°ë°˜ì˜ ì •ì±… ì‹ ê²½ë§ì„ í†µí•´ í–‰ë™(action)ì„ ì‚°ì¶œí•˜ê³ ,  
> ê·¸ ê²°ê³¼ë¥¼ ì‹¤ì‹œê°„ ë³´ìƒ(reward)ìœ¼ë¡œ ë°˜ì˜í•˜ì—¬ ì§€ì†ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.  
>  
> ì •ì±…ì€ ê³ ì •ëœ ê·œì¹™ ê¸°ë°˜ì´ ì•„ë‹ˆë¼,  
> **ì‹œì¥ ë³€ë™ì„±ê³¼ ë¦¬ìŠ¤í¬ ìš”ì¸ì— ë”°ë¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ì¬í•™ìŠµ(adaptive fine-tuning)** ë˜ëŠ” êµ¬ì¡°ì…ë‹ˆë‹¤.

---

### âœ³ï¸ ì£¼ìš” íŠ¹ì§•
- **Online RL Architecture:** ì‹œë®¬ë ˆì´ì…˜ê³¼ ì‹¤ê±°ë˜ ì¤‘ ëª¨ë‘ í•™ìŠµ ê°€ëŠ¥  
- **Dual-Timescale Input:** 15ë¶„ë´‰ / ì¼ë´‰ / ê¹€í”„ ì‹œê³„ì—´ ë³‘ë ¬ ì¸ì½”ë”©  
- **Attention-Based Fusion:** ì‹œê°„ì  ì¤‘ìš”ë„ì— ë”°ë¥¸ ë™ì  ê°€ì¤‘  
- **Policy / Value Head ë¶„ë¦¬:** Softmax(2) + Value(1) êµ¬ì¡°  
- **Lightweight Integration:** ì„œë²„ ì—†ì´ ë¡œì»¬ í™˜ê²½(env)ê³¼ ì§ì ‘ ì—°ë™  

---

### ğŸ§­ í”„ë¡œì íŠ¸ ëª©ì 
- ê¹€ì¹˜í”„ë¦¬ë¯¸ì—„ ë° í™˜ìœ¨ì˜ ë™ì  ë³€í™”ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ë°˜ì˜í•´  
  **ë‹¨ê¸° ì‹œì¥ ì™œê³¡(temporal arbitrage)** ì„ í¬ì°©í•˜ê³  ë°˜ì‘í•˜ëŠ” ê°•í™”í•™ìŠµ ê¸°ë°˜ ì˜ì‚¬ê²°ì • ì—”ì§„ ê°œë°œ  
- ë‹¨ìˆœí•œ ì˜ˆì¸¡í˜• ëª¨ë¸ì´ ì•„ë‹ˆë¼,  
  **í–‰ë™ ê¸°ë°˜ í”¼ë“œë°± ë£¨í”„(actionâ€“rewardâ€“update)** ë¥¼ í†µí•´  
  **ìŠ¤ìŠ¤ë¡œ ì ì‘í•˜ëŠ” ê±°ë˜ ì •ì±…(self-adaptive trading policy)** êµ¬í˜„

---

## ğŸ“‚ í”„ë¡œì íŠ¸ êµ¬ì¡°

```text
agent/
â”œâ”€â”€ config.yaml                     # ê³µí†µ ì„¤ì • (model/ppo/training/policy/data)
â”œâ”€â”€ run_sim.py                      # ì‹œë®¬ í•™ìŠµ ì—”íŠ¸ë¦¬í¬ì¸íŠ¸
â”œâ”€â”€ run_live.py                     # ì‹¤ì „(ì˜¨ë¼ì¸ RL) ì—”íŠ¸ë¦¬í¬ì¸íŠ¸
â”‚
â”œâ”€â”€ core/                           # ğŸ’  ì—ì´ì „íŠ¸ ì½”ì–´ (ì‹œë®¬/ì‹¤ì „ ê³µí†µ)
â”‚   â”œâ”€â”€ agent_core.py               # RLAgent: act()/compute_reward()/learn() + ë²„í¼ ê´€ë¦¬
â”‚   â”œâ”€â”€ model.py                    # GRU + Attention + Policy(2) / Value(1)
â”‚   â”œâ”€â”€ ppo_core.py                 # PPO í´ë¦½ ì†ì‹¤/ì—”íŠ¸ë¡œí”¼/ê°’í•¨ìˆ˜ + optimizer
â”‚   â”œâ”€â”€ memory.py                   # ê°„ë‹¨í•œ ReplayBuffer(rolling)
â”‚   â””â”€â”€ utils.py                    # log(), load_config(), save_path() ë“± ìœ í‹¸
â”‚
â”œâ”€â”€ sim/                            # ğŸ§  ì‹œë®¬ë ˆì´ì…˜(í›ˆë ¨) ì „ìš©
â”‚   â”œâ”€â”€ simulator.py                # ê³¼ê±°ë°ì´í„° ê¸°ë°˜ ê°€ìƒì²´ê²° + ìƒíƒœ/metrics ë°˜í™˜
â”‚   â””â”€â”€ trainer.py                  # ì‹œë®¬ í•™ìŠµ ë£¨í”„(Agentâ†”Simulator ì—°ê²°)
â”‚
â””â”€â”€ live/                           # âš¡ ì‹¤ì „ ë£¨í”„(í”¼ë“œ/ì‹¤í–‰ì€ ì™¸ë¶€ ëª¨ë“ˆ ì£¼ì…)
    â””â”€â”€ live_loop.py                # get_live_state/execute_action ì£¼ì…í˜• ì˜¨ë¼ì¸ RL ë£¨í”„
```


---

## âš™ï¸ ì£¼ìš” ê¸°ëŠ¥

### `/act`
- ì…ë ¥: ìµœê·¼ ì‹œì„¸, ê³„ì¢Œ ìƒíƒœ, ê¹€í”„ ë“±  
- ì¶œë ¥: `action (0=Hold, 1=Action)`  
- ì‚¬ìš©ì²˜: ì‹¤ì „/ì‹œë®¬ ëª¨ë‘ ë™ì¼

### `/learn`
- ì…ë ¥: transition mini-batch  
- ì²˜ë¦¬: PPO ì—…ë°ì´íŠ¸  
- ì¶œë ¥: ì—…ë°ì´íŠ¸ ë¡œê·¸ (loss, reward í‰ê·  ë“±)

### `/save`, `/load`
- ì •ì±… ë²„ì „ ê´€ë¦¬ (`/checkpoints/v1_0_0.pt` ë“±)

### `/set_hparams`
- íƒí—˜ë¥ (temperature), lr, clip ë“± ëŸ°íƒ€ì„ ì¡°ì •

---

## ğŸ§  Model Architecture


```text
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Input:
 â”œâ”€ 15m close series  â†’ GRU + Attention Pooling
 â”œâ”€ 1d close series   â†’ GRU + Attention Pooling
 â”œâ”€ 1d Kimchi Premium    â†’ GRU + Attention Pooling
 â”œâ”€ Account State     â†’ Dense Encoding (KRW, USDT)
 â””â”€ Current Price     â†’ Scalar Input

Fusion:
 â””â”€ Concat + Dense â†’
      â”œâ”€ Policy Head â†’ Softmax(2)  (Hold / Trade)
      â””â”€ Value Head  â†’ Linear(1)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```


## Action semantics
```text
0: HOLD

1: SIGNAL (â†’ ê³„ì¢Œ ë¹„ì¤‘ì— ë”°ë¼ BUY / SELLë¡œ ìë™ í•´ì„)
```



---

## ğŸ§© í•™ìŠµ ì„¤ì •

| **Category**                 | **Key**                         | **Default / Example** | **Description**                |
| ---------------------------- | ------------------------------- | --------------------- | ------------------------------ |
| **Model**                    | `model.hidden_dim`              | `64`                  | GRU hidden state í¬ê¸°            |
|                              | `model.gru_layers`              | `1`                   | GRU layer ìˆ˜                    |
|                              | `model.dropout`                 | `0.1`                 | Dropout ë¹„ìœ¨                     |
|                              | `model.attention`               | `true`                | Attention pooling ì‚¬ìš© ì—¬ë¶€        |
|                              | `model.action_dim`              | `2`                   | í–‰ë™ ì°¨ì› (`BUY`, `SELL`)          |
| **PPO (ê³µí†µ)**                 | `ppo.gamma`                     | `0.99`                | Discount factor (ë³´ìƒ ê°ì‡ ìœ¨)       |
|                              | `ppo.batch_size`                | `32`                  | ë¯¸ë‹ˆë°°ì¹˜ í¬ê¸°                        |
| **Training (ê³µí†µ)**            | `training.device`               | `"cuda"`              | ì—°ì‚° ë””ë°”ì´ìŠ¤ (`cpu`/`cuda`)         |
|                              | `training.buffer_maxlen`        | `10000`               | ê²½í—˜ ë²„í¼ ìµœëŒ€ í¬ê¸°                    |
|                              | `training.save_dir`             | `"checkpoints/"`      | ëª¨ë¸ ì €ì¥ ê²½ë¡œ                       |
|                              | `training.save_interval_steps`  | `500`                 | ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì£¼ê¸°(step ë‹¨ìœ„)           |
| **Training â€“ Simulation**    | `training.sim.lr`               | `3e-4`                | ì‹œë®¬ë ˆì´ì…˜ í•™ìŠµë¥                       |
|                              | `training.sim.clip_epsilon`     | `0.2`                 | PPO í´ë¦¬í•‘ í•œê³„                     |
|                              | `training.sim.entropy_coef`     | `0.01`                | ì—”íŠ¸ë¡œí”¼(íƒí—˜) ê°€ì¤‘ì¹˜                   |
|                              | `training.sim.value_coef`       | `0.5`                 | Value ì†ì‹¤ ê°€ì¤‘ì¹˜                   |
|                              | `training.sim.update_epochs`    | `3`                   | í•™ìŠµ ë°˜ë³µ(epoch) ìˆ˜                 |
|                              | `training.sim.update_interval`  | `64`                  | ì—…ë°ì´íŠ¸ ì£¼ê¸° (ë²„í¼ ê¸¸ì´ ê¸°ì¤€)             |
|                              | `training.sim.update_freq`      | `5`                   | í•™ìŠµ í˜¸ì¶œ ê°„ê²© (step ê¸°ì¤€)             |
| **Training â€“ Live (Online)** | `training.live.lr`              | `1e-4`                | ì‹¤ì „ìš© í•™ìŠµë¥                         |
|                              | `training.live.clip_epsilon`    | `0.1`                 | PPO í´ë¦¬í•‘ í•œê³„                     |
|                              | `training.live.entropy_coef`    | `0.02`                | ì—”íŠ¸ë¡œí”¼(íƒí—˜) ê°€ì¤‘ì¹˜                   |
|                              | `training.live.value_coef`      | `0.4`                 | Value ì†ì‹¤ ê°€ì¤‘ì¹˜                   |
|                              | `training.live.update_epochs`   | `1`                   | ì˜¨ë¼ì¸ ì—…ë°ì´íŠ¸ ë°˜ë³µ ìˆ˜                  |
|                              | `training.live.update_interval` | `16`                  | ì—…ë°ì´íŠ¸ ì£¼ê¸° (ë²„í¼ ê¸¸ì´ ê¸°ì¤€)             |
|                              | `training.live.update_freq`     | `3`                   | í•™ìŠµ í˜¸ì¶œ ê°„ê²© (step ê¸°ì¤€)             |
| **Live Execution**           | `live.refresh_interval`         | `10`                  | ì‹¤ì‹œê°„ ë°ì´í„° ì—…ë°ì´íŠ¸ ê°„ê²©(ì´ˆ)             |
| **Policy Logic**             | `policy.signal_sell_threshold`  | `0.10`                | USDT ì”ê³  ë¹„ì¤‘ì´ 10% ì´ìƒì´ë©´ `SELL` ì‹ í˜¸ |
| **Data Window**              | `data.window_15m`               | `15`                  | 15ë¶„ë´‰ ì…ë ¥ ì‹œí€€ìŠ¤ ê¸¸ì´                 |
|                              | `data.window_1d`                | `10`                  | ì¼ë´‰ ì…ë ¥ ì‹œí€€ìŠ¤ ê¸¸ì´                   |
| **Simulation Env**           | `sim.max_steps`                 | `1000`                | ì‹œë®¬ ìµœëŒ€ step ìˆ˜                   |
|                              | `sim.fee`                       | `0.001`               | ê±°ë˜ ìˆ˜ìˆ˜ë£Œ ë¹„ìœ¨                      |
|                              | `sim.init_krw`                  | `1000000`             | ì´ˆê¸° ìë³¸ (KRW)                    |
| **Versioning**               | `versioning.save_dir`           | `"checkpoints/"`      | ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ê²½ë¡œ                    |
|                              | `versioning.auto_timestamp`     | `true`                | íƒ€ì„ìŠ¤íƒ¬í”„ë³„ í´ë” ìë™ ìƒì„± ì—¬ë¶€             |



---

## ğŸš€ ì‹¤í–‰ ë°©ë²•

1. **Simulated Training**
   ```python
   python run_sim.py
   ```
2. **Live (Online RL)**
   ```python
   python run_live.py
   ```

---

## ğŸ§‘â€ğŸ’» Author

- **ì´ìŠ¹ì—½** â€” RL ë¸Œë ˆì¸ ì„¤ê³„ ë° ì—°êµ¬

---

> _â€œA demon of capitalism, swimming the net and devouring volatility.â€_
