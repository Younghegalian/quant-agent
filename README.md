# **USDT-RL: Online Reinforcement Learning Engine for K-Premia**

> í•´ë‹¹ í”„ë¡œì íŠ¸ëŠ” **Python 3.10+** ê¸°ë°˜ì˜ **Online Reinforcement Learning (RL)** ì—ì´ì „íŠ¸ ì—”ì§„ìœ¼ë¡œ,  
> ê¹€ì¹˜í”„ë¦¬ë¯¸ì—„(K-Premia) ë° í™˜ìœ¨ ë³€ë™ì„ ì´ìš©í•œ **USDTâ€“KRW ë‹¨íƒ€ ì „ëµ**ì„  
> ì‹¤ì‹œê°„ìœ¼ë¡œ í•™ìŠµí•˜ê³  ì ì‘í•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.  
>  
> ê±°ë˜ í™˜ê²½(Environment, K-Premia ëª¨ë“ˆ)ìœ¼ë¡œë¶€í„° ìƒíƒœ(state)ë¥¼ ì£¼ê¸°ì ìœ¼ë¡œ ì…ë ¥ë°›ì•„,  
> GRU + Attention ê¸°ë°˜ì˜ ì •ì±… ì‹ ê²½ë§ì„ í†µí•´ í–‰ë™(action)ì„ ì‚°ì¶œí•˜ê³ ,  
> ê·¸ ê²°ê³¼ë¥¼ ì‹¤ì‹œê°„ ë³´ìƒ(reward)ìœ¼ë¡œ ë°˜ì˜í•˜ì—¬ ì§€ì†ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.  
>  
> ì •ì±…ì€ ê³ ì •ëœ ê·œì¹™ ê¸°ë°˜ì´ ì•„ë‹ˆë¼,  
> **ì‹œì¥ ë³€ë™ì„±ê³¼ ë¦¬ìŠ¤í¬ ìš”ì¸ì— ë”°ë¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ì¬í•™ìŠµ(adaptive fine-tuning)** ë˜ëŠ” êµ¬ì¡°ì…ë‹ˆë‹¤.

---

### âœ³ï¸ ì£¼ìš” íŠ¹ì§•
- **Online RL Architecture:** ì‹œë®¬ë ˆì´ì…˜ê³¼ ì‹¤ê±°ë˜ ì¤‘ ëª¨ë‘ í•™ìŠµ ê°€ëŠ¥  
- **Dual-Timescale Input:** 15ë¶„ë´‰ / ì¼ë´‰ / ê¹€í”„ ì‹œê³„ì—´ ë³‘ë ¬ ì¸ì½”ë”©  
- **Attention-Based Fusion:** ì‹œê°„ì  ì¤‘ìš”ë„ì— ë”°ë¥¸ ë™ì  ê°€ì¤‘  
- **Policy / Value Head ë¶„ë¦¬:** Softmax(2) + Value(1) êµ¬ì¡°  
- **Lightweight Integration:** ì„œë²„ ì—†ì´ ë¡œì»¬ í™˜ê²½(env)ê³¼ ì§ì ‘ ì—°ë™  

---

### ğŸ§­ í”„ë¡œì íŠ¸ ëª©ì 
- ê¹€ì¹˜í”„ë¦¬ë¯¸ì—„ ë° í™˜ìœ¨ì˜ ë™ì  ë³€í™”ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ë°˜ì˜í•´  
  **ë‹¨ê¸° ì‹œì¥ ì™œê³¡(temporal arbitrage)** ì„ í¬ì°©í•˜ê³  ë°˜ì‘í•˜ëŠ” ê°•í™”í•™ìŠµ ê¸°ë°˜ ì˜ì‚¬ê²°ì • ì—”ì§„ ê°œë°œ  
- ë‹¨ìˆœí•œ ì˜ˆì¸¡í˜• ëª¨ë¸ì´ ì•„ë‹ˆë¼,  
  **í–‰ë™ ê¸°ë°˜ í”¼ë“œë°± ë£¨í”„(actionâ€“rewardâ€“update)** ë¥¼ í†µí•´  
  **ìŠ¤ìŠ¤ë¡œ ì ì‘í•˜ëŠ” ê±°ë˜ ì •ì±…(self-adaptive trading policy)** êµ¬í˜„

---

## ğŸ“‚ í”„ë¡œì íŠ¸ êµ¬ì¡°

```text
agent/                                  
 â”œâ”€â”€ __init__.py                         
 â”œâ”€â”€ agent_core.py                       # RLAgent ì •ì˜ (act(), learn())
 â”œâ”€â”€ model.py                            # GRU + Attention + Dual Head êµ¬ì¡°
 â”œâ”€â”€ ppo_core.py                         # PPO í•™ìŠµ ë£¨í”„ (ì„ íƒì‚¬í•­)
 â”œâ”€â”€ state_schema.py                     # ì…ë ¥ ì „ì²˜ë¦¬, normalization, feature shaping
 â”œâ”€â”€ utils.py                            # ë¡œê·¸, ìŠ¤ì¼€ì¤„ëŸ¬, ë©”íŠ¸ë¦­ ê³„ì‚° ë“±
 â””â”€â”€ config.yaml                         # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • íŒŒì¼
```

> ğŸ’¡ *ì‹œë®¬ë ˆì´í„°(`env/`)ëŠ” ë³„ë„ë¡œ êµ¬ì„± ì˜ˆì •. RL ë¸Œë ˆì¸ì€ í™˜ê²½ê³¼ ë…ë¦½ì ìœ¼ë¡œ ì‘ë™*

---

## âš™ï¸ ì£¼ìš” ê¸°ëŠ¥

### `/act`
- ì…ë ¥: ìµœê·¼ ì‹œì„¸, ê³„ì¢Œ ìƒíƒœ, ê¹€í”„ ë“±  
- ì¶œë ¥: `action (0=Hold, 1=Action)`  
- ì‚¬ìš©ì²˜: ì‹¤ì „/ì‹œë®¬ ëª¨ë‘ ë™ì¼

### `/learn`
- ì…ë ¥: transition mini-batch  
- ì²˜ë¦¬: PPO ì—…ë°ì´íŠ¸  
- ì¶œë ¥: ì—…ë°ì´íŠ¸ ë¡œê·¸ (loss, reward í‰ê·  ë“±)

### `/save`, `/load`
- ì •ì±… ë²„ì „ ê´€ë¦¬ (`/checkpoints/v1_0_0.pt` ë“±)

### `/set_hparams`
- íƒí—˜ë¥ (temperature), lr, clip ë“± ëŸ°íƒ€ì„ ì¡°ì •

---

## ğŸ§  Model Architecture


```text
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   15m / 1d / ê¹€í”„ ì‹œê³„ì—´ ì…ë ¥
         â”‚
         â–¼
  GRU Encoders Ã—3
         â”‚
         â–¼
  Attention Pooling
         â”‚
         â–¼
  Dense Fusion  â†  ê³„ì¢Œ ìƒíƒœ + í˜„ì¬ê°€
         â”‚
         â”œâ”€â–¶ Policy Head : Softmax(2)   # Action logits
         â””â”€â–¶ Value Head  : Linear(1)    # State value (V)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

Light temporal encoding Ã— contextual fusion â†’ dual-head decision network.


---

---

## ğŸ§© í•™ìŠµ ì„¤ì •

| íŒŒë¼ë¯¸í„° | ê¸°ë³¸ê°’ | ì„¤ëª… |
|-----------|---------|------|
| Î³ (discount) | 0.995 | ì¥ê¸°ë³´ìƒ ê°€ì¤‘ì¹˜ |
| Î» (GAE) | 0.95 | advantage decay |
| clip | 0.2 | PPO clipping ratio |
| lr | 3e-4 | í•™ìŠµë¥  |
| entropy_coef | 0.01 | íƒí—˜ë„ ë³´ìƒ |
| value_coef | 0.5 | value loss ê°€ì¤‘ì¹˜ |
| temperature | 0.1 | íƒí—˜ë¥  ì¡°ì ˆìš© softmax ì˜¨ë„ |

---

## ğŸš€ ì‹¤í–‰ ë°©ë²•

1. **ì‹œë®¬ë ˆì´í„°ì™€ í•¨ê»˜ ì‚¬ìš©**
   ```python
    from agent.agent_core import RLAgent
    agent = RLAgent()
    state = env.get_state()      # ì‹œë®¬ë ˆì´í„°ê°€ ë§Œë“  ìƒíƒœ
    action = agent.act(state)    # RL ì •ì±…ìœ¼ë¡œ í–‰ë™ ê²°ì •
    env.step(action)             # ê±°ë˜ ìˆ˜í–‰
   ```

---

## ğŸ§‘â€ğŸ’» Author

- **ì´ìŠ¹ì—½** â€” RL ë¸Œë ˆì¸ ì„¤ê³„ ë° ì—°êµ¬

---

> _â€œA demon of capitalism, swimming the net and devouring volatility.â€_
