import torch
import torch.nn.functional as F
from torch import optim


class PPOCore:
    def __init__(self, model, lr, clip_epsilon, entropy_coef, value_coef, update_epochs, max_grad_norm=1.0):
        self.model = model
        self.lr = lr
        self.clip_epsilon = clip_epsilon
        self.entropy_coef = entropy_coef
        self.value_coef = value_coef
        self.update_epochs = update_epochs
        self.max_grad_norm = max_grad_norm
        self.opt = optim.Adam(self.model.parameters(), lr=self.lr)
        self.optimizer = self.opt

    # ------------------------------------------------------------
    def compute_loss(self, logits, old_logits, actions, advantages, values, returns):
        """
        PPO ÏÜêÏã§ Í≥ÑÏÇ∞ + KL Í∞êÏãú
        logits, old_logits: (B, num_actions)
        actions: (B,1)
        advantages, values, returns: (B,1)
        """
        logits = torch.clamp(logits, -20, 20)
        old_logits = torch.clamp(old_logits, -20, 20)

        # Log prob Í≥ÑÏÇ∞
        new_logp = F.log_softmax(logits, dim=-1)
        old_logp = F.log_softmax(old_logits, dim=-1).detach()

        act_new_logp = new_logp.gather(1, actions)
        act_old_logp = old_logp.gather(1, actions)

        # ratio Í≥ÑÏÇ∞ (log ÌôïÎ•† Ï∞®Ïù¥ ‚Üí exp)
        ratio = torch.exp(act_new_logp - act_old_logp)

        # Clipped surrogate objective
        surr1 = ratio * advantages
        surr2 = torch.clamp(ratio, 1.0 - self.clip_epsilon, 1.0 + self.clip_epsilon) * advantages
        policy_loss = -torch.min(surr1, surr2).mean()

        # Value (critic) loss
        value_loss = F.mse_loss(values, returns)

        # Entropy bonus (ÌÉêÏÉâÏÑ± Ïú†ÏßÄ)
        probs = torch.exp(new_logp)
        entropy = -(probs * new_logp).sum(dim=1).mean()

        # Approximate KL divergence (ÏÉà/Ïù¥Ï†Ñ Ï†ïÏ±Ö Í±∞Î¶¨)
        with torch.no_grad():
            probs_new = F.softmax(logits, dim=-1)
            probs_old = F.softmax(old_logits, dim=-1)
            kl = (probs_old * (torch.log(probs_old + 1e-8) - torch.log(probs_new + 1e-8))).sum(dim=1).mean()

        # Ï¥ù ÏÜêÏã§
        loss = policy_loss + self.value_coef * value_loss - self.entropy_coef * entropy

        # üí° KL Î∞òÌôò
        return loss, kl

    # ------------------------------------------------------------
    def step(self, loss):
        """PPO step"""
        self.opt.zero_grad(set_to_none=True)
        loss.backward()

        if self.max_grad_norm:
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=self.max_grad_norm)

        self.opt.step()